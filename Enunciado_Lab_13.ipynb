{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl0JoW4Eodvi"
   },
   "source": [
    "# **Laboratorio 13: üí® Airflow üí®**\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos</strong></center>\n",
    "\n",
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti√°n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol√°s Ojeda, Melanie Pe√±a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ypG7Fsodvj"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
    "\n",
    "- Nombre de alumno 1:Sivert Escaff\n",
    "- Nombre de alumno 2:Rodrigo Montecino\n",
    "\n",
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/...../)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_P7PCPTodvk"
   },
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- Construcci√≥n de pipelines productivos usando `Airflow`.\n",
    "\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer material del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Reconocer los componentes pricipales de `Airflow` y su funcionamiento.\n",
    "- Poner en pr√°ctica la construcci√≥n de pipelines de `Airflow`.\n",
    "- Automatizar procesos t√≠picos de un proyecto de ciencia de datos mediante `Airflow` y `Docker`.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsfK981Uodvk"
   },
   "source": [
    "# **Introducci√≥n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ilM8YDjodvk"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.gifer.com/SUFL.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zrLPQNBodvk"
   },
   "source": [
    "Nico, un estudiante del Mag√≠ster en Ciencia de Datos, se encuentra en la etapa final de sus estudios. Por un lado, est√° muy contento por haber llegado tan lejos, pero por otro, no puede evitar sentirse inquieto. Desde que ingres√≥ a la universidad, una pregunta lo ha perseguido: ¬øqu√© tan probable es que pueda ser seleccionado en los lugares donde env√≠e postulaciones para puestos de trabajo?\n",
    "\n",
    "Esta duda lo mantiene en constante reflexi√≥n, especialmente porque sabe que el mercado laboral en Ciencia de Datos es competitivo y exige habilidades no solo t√©cnicas, sino tambi√©n estrat√©gicas para destacar. Sin embargo, Nico actualmente est√° completamente enfocado en terminar su tesis de mag√≠ster y ha tenido que postergar cualquier preparaci√≥n espec√≠fica para enfrentar el desaf√≠o de las postulaciones laborales.\n",
    "\n",
    "Al ver el avance y las habilidades que usted ha demostrado en el curso, Nico decidi√≥ proponerle un desaf√≠o que le permitir√° disminuir la incertidumbre sobre su futuro laboral. Inspirado en sus conocimientos, √©l recolect√≥ un conjunto de datos que contiene informaci√≥n sobre diversos factores que influyen en las decisiones de contrataci√≥n de empresas al seleccionar entre sus postulantes. Este set de datos incluye los siguientes atributos:\n",
    "\n",
    "- Age: Edad del candidato\n",
    "- Gender: Genero del candidato. Male (0), Female (1).\n",
    "- EducationLevel: Mayor nivel educacional alcanzado por el candidato. Licenciatura Tipo 1 (1), Licenciatura Tipo 2 (2), Maestr√≠a (3), PhD. (4).\n",
    "- ExperienceYears: A√±os de experiencia profesional.\n",
    "- PreviousCompanies: Numero de compa√±√≠as donde el candidato ha trabajado anteriormente.\n",
    "- DistanceFromCompany: Distancia en kilometros entre la residencia del candidato y la compa√±√≠a donde postula.\n",
    "- InterviewScore: Puntaje obtenido en la entrevista por el candidato entre 0 a 100.\n",
    "- SkillScore: Puntaje obtenido en evaluaci√≥n de habilidades t√©cnicas por el candidato, entre 0 a 100.\n",
    "- PersonalityScore: Puntaje obtenido en pruebas de personalidad del candidato, entre 0 a 100.\n",
    "- RecruitmentStrategy: Estrategia del equipo de reclutamiento. Agresiva (1), Moderada (2), Conservadora (3).\n",
    "\n",
    "Variable a predecir:\n",
    "- HiringDecision: Resultado de la postulaci√≥n. No contratado (0), Contratado (1).\n",
    "\n",
    "Su objetivo ser√° ayudar a Nico a desarrollar un modelo que le permita predecir, basado en estos factores, si un postulante ser√° contratado o no. Esta herramienta no solo le dar√° a Nico mayor claridad sobre el impacto de ciertos atributos en la decisi√≥n final de contrataci√≥n, sino que tambi√©n le permitir√° aplicar sus conocimientos de Ciencia de Datos para resolver una pregunta que a muchos estudiantes como √©l les inquieta.\n",
    "\n",
    "Como estudiante del curso Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos, deber√° demostrar sus capacidades para preprocesar, analizar y modelar datos, brind√°ndole a Nico una soluci√≥n robusta y bien fundamentada para su problem√°tica.\n",
    "\n",
    "`Nota:` El siguiente [enlace](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data/data) contiene el set de datos original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yeh268atodvl"
   },
   "source": [
    "# **1. Pipeline de Predicci√≥n Lineal** (30 Puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmB1LTWnodvl"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://c.tenor.com/WvHhQt2UpuAAAAAd/wolf-of-wall-street.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF1bTY0Modvl"
   },
   "source": [
    "En esta secci√≥n buscaremos desplegar un producto utilizando un modelo de clasificaci√≥n `Random Forest` para determinar si una persona ser√° contratada o no en un proceso de selecci√≥n. Para ello, comenzaremos preparando un pipeline lineal mediante `Airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7MllF4fodvl"
   },
   "source": [
    "## **1.1 Preparando el Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1JxaZgModvl"
   },
   "source": [
    "**Primero, aseg√∫rese de tener creada las carpetas `dags`, `plugins` y `logs`**.\n",
    "\n",
    "Comenzamos preparando un archivo llamado `hiring_functions.py`, el cual guardar√° en la carpeta `dags` y debe contener lo siguiente:\n",
    "\n",
    "1. (3 puntos) Una funci√≥n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci√≥n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - splits\n",
    "  - models\n",
    "\n",
    "  `Hint`: Puede hacer uso de kwargs para obtener la fecha de ejecuci√≥n mediante el DAG. El siguiente [Enlace](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) le puede ser √∫til.\n",
    "2. (3 puntos) Una funci√≥n llamada `split_data()` que lea el archivo `data_1.csv` de la carepta `raw` y a partir de este, aplique un *hold out*, generando un dataset de entrenamiento y uno de prueba. Luego debe guardar estos nuevos conjuntos de datos en la carpeta `splits`. `Nota:` Utilice un 20% para el conjunto de prueba, mantenga la proporci√≥n original en la variable objetivo y fije una semilla.\n",
    "3. (8 puntos) Cree una funci√≥n llamada `preprocess_and_train()` que:\n",
    "  - Lea los set de entrenamiento y prueba de la carpeta `splits`.\n",
    "  - Cree y aplique un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes. Puede apoyarse del archivo `data_1_report.html` para justificar cualquier paso del preprocesamiento.\n",
    "  \n",
    "  - A√±ada una etapa de entrenamiento utilizando el modelo `RandomForest`.\n",
    "  \n",
    "  Esta funci√≥n **debe crear un archivo `joblib` (an√°logo a `pickle`) con el pipeline entrenado** en la carepta `models`, adem√°s debe **imprimir** el accuracy en el conjunto de prueba y el f1-score de la clase positiva (contratado).\n",
    "3. (1 punto) Incorpore la funci√≥n `gradio_interface` en su script, modificando la ruta de acceso a su modelo, de forma que pueda leerlo desde la carepta `models`. Puede realizar modificacioneds adicionales en caso de ser necesario.\n",
    "\n",
    "`NOTA:` Se permite la creaci√≥n de funciones auxiliares si lo estiman conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1732807326416,
     "user": {
      "displayName": "Eduardo Andr√©s Moya Briones",
      "userId": "15114984037692151374"
     },
     "user_tz": 180
    },
    "id": "ze9Iotloodvl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folders():\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    os.makedirs(f'dags/{today}/raw', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/splits', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/models', exist_ok=True)\n",
    "\n",
    "create_folders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train.to_csv(f'dags/{datetime.today().strftime(\"%Y-%m-%d\")}/splits/train.csv', index=False)\n",
    "    test.to_csv(f'dags/{datetime.today().strftime(\"%Y-%m-%d\")}/splits/test.csv', index=False)\n",
    "\n",
    "split_data('dags/2024-12-06/raw/recruitment_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K_RCVPnUodvm"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from joblib import dump\n",
    "\n",
    "def preprocess_and_train():\n",
    "    data = pd.read_csv(f'dags/{datetime.today().strftime(\"%Y-%m-%d\")}/splits/train.csv')\n",
    "    X = data.drop('HiringDecision', axis=1)\n",
    "    y = data['HiringDecision']\n",
    "    \n",
    "    numeric_features = ['Age', 'ExperienceYears', 'DistanceFromCompany', 'SkillScore', 'PersonalityScore']\n",
    "    categorical_features = ['Gender', 'EducationLevel', 'PreviousCompanies', 'InterviewScore', 'RecruitmentStrategy']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    dump(clf, f'dags/{datetime.today().strftime(\"%Y-%m-%d\")}/models/model.joblib')\n",
    "\n",
    "preprocess_and_train()\n",
    "\n",
    "\n",
    "def gradio_interface():\n",
    "\n",
    "    model_path= 'dags/2024-12-06/models/model.joblib'\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=lambda file: predict(file, model_path),\n",
    "        inputs=gr.File(label=\"Sube un archivo JSON\"),\n",
    "        outputs=\"json\",\n",
    "        title=\"Hiring Decision Prediction\",\n",
    "        description=\"Sube un archivo JSON con las caracter√≠sticas de entrada para predecir si Nico ser√° contratado o no.\"\n",
    "    )\n",
    "    interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTKOj1hfodvm"
   },
   "source": [
    "## **1.2 Creando Nuestro DAG** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkEZcEh4odvm"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExMzNjd3hxOWIzZjhwZDc5NnJwZzZodnNrbWI5cGtjY2VwZjI0eDdnOSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Dh5q0sShxgp13DwrvG/giphy.webp\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-MTaxTgodvm"
   },
   "source": [
    "Con las funciones del pipeline ya creadas, ahora vamos a proceder a crear un Directed Acyclic Graph (DAG). Para ello, se le pide lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-yUak2Rodvm"
   },
   "source": [
    "- (10 puntos) Cree un segundo archivo llamado `dag_lineal.py` y guardelo en la carpeta dags. Este script debe seguir la siguiente estructura (Ver imagen de referencia):\n",
    "\n",
    "    0. Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, ejecuci√≥n manual y **sin backfill**. Asigne un `dag_id` que pueda reconocer facilmente, como `hiring_lineal`, etc.\n",
    "    1. Debe comenzar con un marcador de posici√≥n que indique el inicio del pipeline.\n",
    "    2. Cree una carpeta correspondiente a la ejecuci√≥n del pipeline y cree las subcarpetas `raw`, `splits` y `models` mediante la funci√≥n `create_folders()`.\n",
    "    3. Debe descargar el archivo `data_1.csv` del siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv). Debe guardar el archivo en la carpeta raw de la ejecuci√≥n correspondiente.`Hint:` Le puede ser √∫til el comando `curl -o <path de guardado> <enlace con los datos>`.\n",
    "    4. Debe aplicar un hold out mediante la funci√≥n `split_data()` de su archivo creado en la subsecci√≥n anterior.\n",
    "    5. Debe aplicar el preprocesamiento y el entrenamiento del modelo mediante la funci√≥n `preprocess_and_train()`.\n",
    "    6. Finalmente, debe montar una interfaz en gradio donde pueda cargar un archivo ``json``.\n",
    "\n",
    "\n",
    "- (3 puntos) Cree un `DockerFile` para montar un contenedor que contenga Airflow. Adicionalmente, cree una carpeta llamada dags donde guardar√° el script.py creado anteriormente.\n",
    "\n",
    "    `Nota:` Para la imagen, se recomienda utilizar python 3.10-slim. Adicionalmente, puede instalar `curl` mediante la siguiente linea de c√≥digo: `RUN apt-get update && apt-get install -y curl`.\n",
    "\n",
    "- Construya el contenedor en Docker y acceda a la aplicaci√≥n web de Airflow mediante el siguiente [enlace](http://localhost:8080/). Inicie sesi√≥n, acceda al DAG creado y ejecute de forma manual su pipeline.\n",
    "\n",
    "- (2 puntos) Acceda a la URL p√∫blica de Gradio e ingrese el archivo `nico_data.json` a su modelo. ¬øQue predicci√≥n entreg√≥ el modelo para Nico? Adjunte im√°genes de su resultado. `Hint:` Puede acceder a los `logs` para obtener los prints y la URL p√∫blica.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci√≥n `ds`.\n",
    "\n",
    "**Para esta secci√≥n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser√°n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im√°genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiMTgQfJpuIv"
   },
   "source": [
    "DAG de referencia:\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1iwDgECZfFeWq1dl433tMa6_3CNF9cn1L\" width=\"1200\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-airflow==2.7.0\n",
      "  Downloading apache_airflow-2.7.0-py3-none-any.whl.metadata (121 kB)\n",
      "Collecting alembic<2.0,>=1.6.3 (from apache-airflow==2.7.0)\n",
      "  Downloading alembic-1.11.3-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting argcomplete>=1.10 (from apache-airflow==2.7.0)\n",
      "  Downloading argcomplete-3.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting asgiref (from apache-airflow==2.7.0)\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: attrs>=22.1.0 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (23.1.0)\n",
      "Requirement already satisfied: blinker in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (1.6.2)\n",
      "Collecting cattrs>=22.1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading cattrs-23.1.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting colorlog<5.0,>=4.0.2 (from apache-airflow==2.7.0)\n",
      "  Downloading colorlog-4.8.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting configupdater>=3.1.1 (from apache-airflow==2.7.0)\n",
      "  Downloading ConfigUpdater-3.1.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting connexion>=2.10.0 (from connexion[flask]>=2.10.0->apache-airflow==2.7.0)\n",
      "  Downloading connexion-2.14.2-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting cron-descriptor>=1.2.24 (from apache-airflow==2.7.0)\n",
      "  Downloading cron_descriptor-1.4.0.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting croniter>=0.3.17 (from apache-airflow==2.7.0)\n",
      "  Downloading croniter-1.4.1-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Collecting cryptography>=0.9.3 (from apache-airflow==2.7.0)\n",
      "  Downloading cryptography-41.0.3-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting deprecated>=1.2.13 (from apache-airflow==2.7.0)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dill>=0.2.2 (from apache-airflow==2.7.0)\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting flask<2.3,>=2.2 (from apache-airflow==2.7.0)\n",
      "  Downloading Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting flask-appbuilder==4.3.3 (from apache-airflow==2.7.0)\n",
      "  Downloading Flask_AppBuilder-4.3.3-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting flask-caching>=1.5.0 (from apache-airflow==2.7.0)\n",
      "  Downloading Flask_Caching-2.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting flask-login>=0.6.2 (from apache-airflow==2.7.0)\n",
      "  Downloading Flask_Login-0.6.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting flask-session>=0.4.0 (from apache-airflow==2.7.0)\n",
      "  Downloading flask_session-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting flask-wtf>=0.15 (from apache-airflow==2.7.0)\n",
      "  Downloading Flask_WTF-1.1.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting google-re2>=1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading google_re2-1.1-6-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting graphviz>=0.12 (from apache-airflow==2.7.0)\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting gunicorn>=20.1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpx (from apache-airflow==2.7.0)\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting itsdangerous>=2.0 (from apache-airflow==2.7.0)\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jinja2>=3.0.0 (from apache-airflow==2.7.0)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting jsonschema>=4.18.0 (from apache-airflow==2.7.0)\n",
      "  Downloading jsonschema-4.19.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting lazy-object-proxy (from apache-airflow==2.7.0)\n",
      "  Downloading lazy-object-proxy-1.9.0.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting linkify-it-py>=2.0.0 (from apache-airflow==2.7.0)\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting lockfile>=0.12.2 (from apache-airflow==2.7.0)\n",
      "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown>=3.0 (from apache-airflow==2.7.0)\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markdown-it-py>=2.1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: markupsafe>=1.1.1 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (2.1.3)\n",
      "Collecting marshmallow-oneofschema>=2.0.1 (from apache-airflow==2.7.0)\n",
      "  Downloading marshmallow_oneofschema-3.0.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting mdit-py-plugins>=0.3.0 (from apache-airflow==2.7.0)\n",
      "  Downloading mdit_py_plugins-0.4.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-api==1.15.0 (from apache-airflow==2.7.0)\n",
      "  Downloading opentelemetry_api-1.15.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp (from apache-airflow==2.7.0)\n",
      "  Downloading opentelemetry_exporter_otlp-1.15.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting packaging>=14.0 (from apache-airflow==2.7.0)\n",
      "  Downloading packaging-23.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from apache-airflow==2.7.0)\n",
      "  Downloading pathspec-0.11.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pendulum>=2.0 (from apache-airflow==2.7.0)\n",
      "  Downloading pendulum-2.1.2.tar.gz (81 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pluggy>=1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading pluggy-1.2.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting psutil>=4.2.0 (from apache-airflow==2.7.0)\n",
      "  Downloading psutil-5.9.5-cp36-abi3-win_amd64.whl.metadata (21 kB)\n",
      "Collecting pydantic<2.0.0,>=1.10.0 (from apache-airflow==2.7.0)\n",
      "  Downloading pydantic-1.10.12-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pygments>=2.0.1 (from apache-airflow==2.7.0)\n",
      "  Downloading Pygments-2.16.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyjwt>=2.0.0 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (2.8.0)\n",
      "Collecting python-daemon>=3.0.0 (from apache-airflow==2.7.0)\n",
      "  Downloading python_daemon-3.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting python-dateutil>=2.3 (from apache-airflow==2.7.0)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting python-nvd3>=0.15.0 (from apache-airflow==2.7.0)\n",
      "  Downloading python-nvd3-0.15.0.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting python-slugify>=5.0 (from apache-airflow==2.7.0)\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: rfc3339-validator>=0.1.4 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (0.1.4)\n",
      "Collecting rich>=12.4.4 (from apache-airflow==2.7.0)\n",
      "  Downloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rich-argparse>=1.0.0 (from apache-airflow==2.7.0)\n",
      "  Downloading rich_argparse-1.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setproctitle>=1.1.8 (from apache-airflow==2.7.0)\n",
      "  Downloading setproctitle-1.3.2.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting sqlalchemy<2.0,>=1.4 (from apache-airflow==2.7.0)\n",
      "  Downloading SQLAlchemy-1.4.49-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading SQLAlchemy_JSONField-1.0.1.post0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.2.0,>=6.2.0 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from apache-airflow==2.7.0) (8.2.3)\n",
      "Collecting termcolor>=1.1.0 (from apache-airflow==2.7.0)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from apache-airflow==2.7.0)\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting unicodecsv>=0.14.1 (from apache-airflow==2.7.0)\n",
      "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting werkzeug>=2.0 (from apache-airflow==2.7.0)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting apache-airflow-providers-common-sql (from apache-airflow==2.7.0)\n",
      "  Downloading apache_airflow_providers_common_sql-1.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting apache-airflow-providers-ftp (from apache-airflow==2.7.0)\n",
      "  Downloading apache_airflow_providers_ftp-3.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting apache-airflow-providers-http (from apache-airflow==2.7.0)\n",
      "  Downloading apache_airflow_providers_http-4.5.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting apache-airflow-providers-imap (from apache-airflow==2.7.0)\n",
      "  Downloading apache_airflow_providers_imap-3.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting apache-airflow-providers-sqlite (from apache-airflow==2.7.0)\n",
      "  Downloading apache_airflow_providers_sqlite-3.4.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting apispec<7,>=6.0.0 (from apispec[yaml]<7,>=6.0.0->flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading apispec-6.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.3.3->apache-airflow==2.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9,>=8 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from flask-appbuilder==4.3.3->apache-airflow==2.7.0) (8.1.7)\n",
      "Collecting email-validator<2,>=1.0.5 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading email_validator-1.3.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Collecting Flask-Babel<3,>=1 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading Flask_Babel-2.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting Flask-Limiter<4,>3 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading Flask_Limiter-3.3.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting Flask-SQLAlchemy<3,>=2.4 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting Flask-JWT-Extended<5.0.0,>=4.0.0 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading Flask_JWT_Extended-4.5.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting marshmallow<4,>=3.18.0 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting marshmallow-sqlalchemy<0.27.0,>=0.22.0 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading marshmallow_sqlalchemy-0.26.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting prison<1.0.0,>=0.2.1 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading prison-0.2.1-py2.py3-none-any.whl.metadata (973 bytes)\n",
      "Collecting sqlalchemy-utils<1,>=0.32.21 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading SQLAlchemy_Utils-0.41.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting WTForms<4 (from flask-appbuilder==4.3.3->apache-airflow==2.7.0)\n",
      "  Downloading WTForms-3.0.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from opentelemetry-api==1.15.0->apache-airflow==2.7.0) (75.1.0)\n",
      "Collecting Mako (from alembic<2.0,>=1.6.3->apache-airflow==2.7.0)\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting clickclick<21,>=1.2 (from connexion>=2.10.0->connexion[flask]>=2.10.0->apache-airflow==2.7.0)\n",
      "  Downloading clickclick-20.10.2-py2.py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: PyYAML<7,>=5.1 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from connexion>=2.10.0->connexion[flask]>=2.10.0->apache-airflow==2.7.0) (6.0.1)\n",
      "Collecting requests<3,>=2.9.1 (from connexion>=2.10.0->connexion[flask]>=2.10.0->apache-airflow==2.7.0)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: inflection<0.6,>=0.3.1 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from connexion>=2.10.0->connexion[flask]>=2.10.0->apache-airflow==2.7.0) (0.5.1)\n",
      "Collecting cffi>=1.12 (from cryptography>=0.9.3->apache-airflow==2.7.0)\n",
      "  Downloading cffi-1.15.1.tar.gz (508 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.13->apache-airflow==2.7.0)\n",
      "  Downloading wrapt-1.15.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting cachelib<0.10.0,>=0.9.0 (from flask-caching>=1.5.0->apache-airflow==2.7.0)\n",
      "  Downloading cachelib-0.9.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow==2.7.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\siver\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->apache-airflow==2.7.0) (0.30.2)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->apache-airflow==2.7.0)\n",
      "  Downloading rpds_py-0.9.2.tar.gz (16 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [6 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "√ó Encountered error while generating package metadata.\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install apache-airflow==2.7.0 --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.7.0/constraints-3.8.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckzDqsF4odvn"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Funci√≥n para inicializar el pipeline (create_folders)\n",
    "def create_folders():\n",
    "    from datetime import datetime\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    os.makedirs(f'dags/{today}/raw', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/splits', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/models', exist_ok=True)\n",
    "\n",
    "# Funci√≥n para descargar el archivo del enlace de arriba \n",
    "def download_data():\n",
    "    url = \"<https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv>\"  \n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    file_path = f'dags/{today}/raw/data_1.csv'\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Funci√≥n para aplicar split_data\n",
    "def split_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    file_path = f'dags/{today}/raw/data_1.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train.to_csv(f'dags/{today}/splits/train.csv', index=False)\n",
    "    test.to_csv(f'dags/{today}/splits/test.csv', index=False)\n",
    "\n",
    "# Funci√≥n para aplicar el pipeline \n",
    "def preprocess_and_train():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from joblib import dump\n",
    "    import pandas as pd\n",
    "    \n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    data = pd.read_csv(f'dags/{today}/splits/train.csv')\n",
    "    X = data.drop('HiringDecision', axis=1)\n",
    "    y = data['HiringDecision']\n",
    "    \n",
    "    numeric_features = ['Age', 'ExperienceYears', 'DistanceFromCompany', \n",
    "                        'SkillScore', 'PersonalityScore']\n",
    "    categorical_features = ['Gender', 'EducationLevel', \n",
    "                            'PreviousCompanies', 'InterviewScore', \n",
    "                            'RecruitmentStrategy']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    dump(clf, f'dags/{today}/models/model.joblib')\n",
    "\n",
    "# Funci√≥n para inicializar la interfaz con gradio\n",
    "def initialize_gradio():\n",
    "    from gradio_interface import gradio_interface\n",
    "    gradio_interface()\n",
    "\n",
    "# Definir el DAG\n",
    "default_args = {\n",
    "    'start_date': datetime(2024, 10, 1),\n",
    "    'catchup': False\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='hiring_lineal',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,\n",
    "    description='Pipeline de predicci√≥n de decisiones de contrataci√≥n',\n",
    "    tags=['example']\n",
    ") as dag:\n",
    "\n",
    "    create_folders_task = PythonOperator(\n",
    "        task_id='create_folders',\n",
    "        python_callable=create_folders\n",
    "    )\n",
    "\n",
    "    download_data_task = PythonOperator(\n",
    "        task_id='download_data',\n",
    "        python_callable=download_data\n",
    "    )\n",
    "\n",
    "    split_data_task = PythonOperator(\n",
    "        task_id='split_data',\n",
    "        python_callable=split_data\n",
    "    )\n",
    "\n",
    "    preprocess_and_train_task = PythonOperator(\n",
    "        task_id='preprocess_and_train',\n",
    "        python_callable=preprocess_and_train\n",
    "    )\n",
    "\n",
    "    initialize_gradio_task = PythonOperator(\n",
    "        task_id='initialize_gradio',\n",
    "        python_callable=initialize_gradio\n",
    "    )\n",
    "\n",
    "    # Configuraciosn\n",
    "    create_folders_task >> download_data_task >> split_data_task >> preprocess_and_train_task >> initialize_gradio_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqBlHcBQpXJb"
   },
   "source": [
    "# **2. Paralelizando el Pipeline** (30 puntos)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/vDv7mn58skcAAAAM/clapping.gif\" width=\"300\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQaVOeiqO_R"
   },
   "source": [
    "Al ver los resultados obtenidos, Nico queda muy contento con el clasificador. Sin embargo, le aparecen algunas dudas respecto al funcionamiento del pipeline. Primero le comenta que es posible que en un futuro tenga nuevos datos que podr√≠an ser √∫tiles para realizar nuevos entrenamientos, por lo que ser√≠a ideal si este pipeline se fuera ejecutando de forma peri√≥dica y no de forma manual. Adem√°s, Nico le menciona que le gustar√≠a explorar el desempe√±o de otros modelos adem√°s de `Random Forest`, de forma que el pipeline seleccione de forma autom√°tica el modelo con mejor desempe√±o para luego hacer la predicci√≥n de Nico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mGPMg0ur-wR"
   },
   "source": [
    "## **2.1 Preparando un Nuevo Pipeline** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpU81VCRr-Hr"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.makeagif.com/media/7-07-2015/oH6WRw.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KcXuS6bsZAw"
   },
   "source": [
    "De acuerdo a lo que le coment√≥ Nico, usted decide crear un nuevo script con las funciones que utilizar√° su pipeline. Por ende, dentro de la carpeta `dags`, usted crear√° el archivo `hiring_dynamic_functions.py` el cual debe contener:\n",
    "\n",
    "1. (2 puntos) Una funci√≥n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci√≥n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
    "  - raw\n",
    "  - preprocessed\n",
    "  - splits\n",
    "  - models\n",
    "2. (2 puntos) Una funci√≥n llamada `load_ands_merge()` que lea desde la carpeta `raw` los archivos `data_1.csv`y `data_2.csv` en caso de estar disponible. Luego concatene estos y genere un nuevo archivo resultante, guard√°ndolo en la carpeta `preprocessed`.\n",
    "\n",
    "3. (2 puntos) Una funci√≥n llamada `split_data()` que lea la data guardada en la carpeta `preprocessed` y realice un hold out sobre esta data. Esta funci√≥n debe crear un conjunto de entrenamiento y uno de prueba. Mantenga una semilla y 20% para el conjunto de prueba. Guarde los conjuntos resultantes en la carpeta `splits`.\n",
    "\n",
    "4. (6 puntos) Una funci√≥n llamada `train_model()` que reciba un modelo de clasificaci√≥n.\n",
    "    - La funci√≥n debe comenzar leyendo el conjunto de entrenamiento desde la carpeta `spits`.\n",
    "    - Esta debe crear y aplicar un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes.\n",
    "    - A√±ada una etapa de entrenamiento utilizando un modelo que ingrese a la funci√≥n.\n",
    "  \n",
    "  Esta funci√≥n **debe crear un archivo joblib con el pipeline entrenado**. Guarde el modelo con un nombre que le permita una facil identificaci√≥n dentro de la carpeta `models`.\n",
    "\n",
    "5. (3 puntos) Una funci√≥n llamada `evaluate_models()` que reciba sus modelos entrenados desde la carpeta `models`, eval√∫e su desempe√±o mediante `accuracy` en el conjunto de prueba y seleccione el mejor modelo obtenido. Luego guarde el mejor modelo como archivo `.joblib`. Su funci√≥n debe imprimir el nombre del modelo seleccionado y el accuracy obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnX61hxjW9rI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Crear carpetas\n",
    "def create_folders():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    os.makedirs(f'dags/{today}/raw', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/preprocessed', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/splits', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/models', exist_ok=True)\n",
    "\n",
    "# 2. Cargar y combinar archivos\n",
    "def load_and_merge():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    file1 = f'dags/{today}/raw/data_1.csv'\n",
    "    file2 = f'dags/{today}/raw/data_2.csv'\n",
    "    data1 = pd.read_csv(file1)\n",
    "    data2 = pd.read_csv(file2)\n",
    "    merged_data = pd.concat([data1, data2], ignore_index=True)\n",
    "    merged_data.to_csv(f'dags/{today}/preprocessed/merged_data.csv', index=False)\n",
    "\n",
    "# 3. Dividir datos\n",
    "def split_data():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    data = pd.read_csv(f'dags/{today}/preprocessed/merged_data.csv')\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train.to_csv(f'dags/{today}/splits/train.csv', index=False)\n",
    "    test.to_csv(f'dags/{today}/splits/test.csv', index=False)\n",
    "\n",
    "# 4. Entrenar modelo\n",
    "def train_model(model_class):\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    train_data = pd.read_csv(f'dags/{today}/splits/train.csv')\n",
    "    X = train_data.drop('HiringDecision', axis=1)\n",
    "    y = train_data['HiringDecision']\n",
    "\n",
    "    numeric_features = ['Age', 'ExperienceYears', 'DistanceFromCompany', \n",
    "                        'SkillScore', 'PersonalityScore']\n",
    "    categorical_features = ['Gender', 'EducationLevel', \n",
    "                            'PreviousCompanies', 'InterviewScore', \n",
    "                            'RecruitmentStrategy']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model_class)\n",
    "    ])\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    model_name = model_class.__class__.__name__\n",
    "    model_path = f'dags/{today}/models/{model_name}.joblib'\n",
    "    dump(clf, model_path)\n",
    "    return model_path\n",
    "\n",
    "# 5. Evaluar modelos\n",
    "def evaluate_models():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    test_data = pd.read_csv(f'dags/{today}/splits/test.csv')\n",
    "    X_test = test_data.drop('HiringDecision', axis=1)\n",
    "    y_test = test_data['HiringDecision']\n",
    "    \n",
    "    model_dir = f'dags/{today}/models'\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_model_name = ''\n",
    "    \n",
    "    for model_file in os.listdir(model_dir):\n",
    "        model_path = os.path.join(model_dir, model_file)\n",
    "        model = load(model_path)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_model_name = model_file\n",
    "    \n",
    "    print(f\"Mejor modelo: {best_model_name} con accuracy: {best_accuracy}\")\n",
    "    dump(best_model, f'dags/{today}/models/best_model.joblib')\n",
    "\n",
    "# Ejecuci√≥n secuencial de las funciones\n",
    "if __name__ == \"__main__\":\n",
    "    create_folders()\n",
    "    load_and_merge()\n",
    "    split_data()\n",
    "    train_model(RandomForestClassifier(random_state=42))\n",
    "    train_model(LogisticRegression(random_state=42))\n",
    "    evaluate_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUYkXWcZJz3b"
   },
   "source": [
    "## **2.2 Componiendo un nuevo DAG** (15 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak7uL9YXJ6Xj"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/977552ab-0b55-4118-9948-06f6386474da_text.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbE6mu20LfWd"
   },
   "source": [
    "Con las nuevas funciones, vamos a crear nuestro nuevo DAG. Para ello, cree un nuevo script en la carpeta `dags`, llamandolo `dag_dynamic.py`. Este script debe contener la siguiente estructura:\n",
    "\n",
    "1. (1 punto) Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, el cual se debe ejecutar el d√≠a 5 de cada mes a las 15:00 UTC. Utilice un `dag_id` interpretable para identificar f√°cilmente. **Habilite el backfill** para que pueda ejecutar tareas programadas desde fechas pasadas.\n",
    "2. (1 punto) Comience con un marcador de posici√≥n que indique el inicio del pipeline.\n",
    "3. (2 puntos) Cree una carpeta correspondiente a la ejecuci√≥n del pipeline y cree las subcarpetas `raw`, `preprocessed`, `splits` y `models` mediante la funci√≥n `create_folders()`.\n",
    "4. (2 puntos) Implemente un `Branching`que siga la siguiente l√≥gica:\n",
    "  - Fechas previas al 1 de noviembre de 2024: Se descarga solo `data_1.csv`\n",
    "  - Desde el 1 de noviembre del 2024: descarga `data_1.csv` y `data_2.csv`.\n",
    "  En el siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv) puede descargar `data_2.csv`.\n",
    "5. (1 punto) Cree una tarea que concatene los datasets disponibles mediante la funci√≥n `load_and_merge()`. Configure un `Trigger` para que la tarea se ejecute si encuentra disponible **como m√≠nimo** uno de los archivos.\n",
    "6. (1 punto) Aplique el hold out al dataset mediante la funci√≥n `split_data()`, obteniendo un conjunto de entrenamiento y uno de prueba.\n",
    "7. (2 puntos) Realice 3 entrenamientos en paralelo:\n",
    "  - Un modelo Random Forest.\n",
    "  - 2 modelos a elecci√≥n.\n",
    "  Aseg√∫rese de guardar sus modelos entrenados con nombres distintivos. Utilice su funci√≥n `train_model()` para ello.\n",
    "8. (2 puntos)Mediante la funci√≥n `evaluate_models()`, eval√∫e los modelos entrenados, registrando el accuracy de cada modelo en el set de prueba. Luego debe imprimir el mejor modelo seleccionado y su respectiva m√©trica. Configure un `Trigger` para que la tarea se ejecute solamente si los 3 modelos fueron entrenados y guardados.\n",
    "\n",
    "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci√≥n `ds`.\n",
    "\n",
    "Una vez creado el script, vuelva a construir el contenedor en Docker, acceda a la aplicaci√≥n web de Airflow, ejecute su pipeline y muestre sus resultados. Adjunte im√°genes que ayuden a mostrar el proceso y sus resultados.\n",
    "\n",
    "Adicionalmente, responda (1 c/u):\n",
    "\n",
    "- ¬øCual es el accuracy de cada modelo en la ejecuci√≥n de octubre? ¬øSe obtienen los mismos resultados a partir de Noviembre?\n",
    "- Analice como afect√≥ el a√±adir datos a sus modelos mediante el desempe√±o del modelo y en costo computacional.\n",
    "- Muestre el esquema de su DAG ejecutado en octubre y en noviembre.\n",
    "\n",
    "\n",
    "`Nota:` Para esta secci√≥n no debe implementar la tarea en gradio, solamente se espera determinar el mejor modelo y comparar el desempe√±o obtenido.\n",
    "\n",
    "**IMPORTANTE: Para esta secci√≥n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser√°n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im√°genes de apoyo, como screenshots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMgK2sKTYJji"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Funci√≥n para descargar el archivo del enlace\n",
    "def download_data():\n",
    "    url = \"<https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv>\"  \n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    file_path = f'dags/{today}/raw/data_2.csv'\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Crear carpetas\n",
    "def create_folders():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    os.makedirs(f'dags/{today}/raw', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/preprocessed', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/splits', exist_ok=True)\n",
    "    os.makedirs(f'dags/{today}/models', exist_ok=True)\n",
    "\n",
    "# 2. Cargar y combinar archivos\n",
    "def load_and_merge():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    file1 = f'dags/{today}/raw/data_1.csv'\n",
    "    file2 = f'dags/{today}/raw/data_2.csv'\n",
    "    data1 = pd.read_csv(file1)\n",
    "    data2 = pd.read_csv(file2)\n",
    "    merged_data = pd.concat([data1, data2], ignore_index=True)\n",
    "    merged_data.to_csv(f'dags/{today}/preprocessed/merged_data.csv', index=False)\n",
    "\n",
    "# 3. Dividir datos\n",
    "def split_data():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    data = pd.read_csv(f'dags/{today}/preprocessed/merged_data.csv')\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train.to_csv(f'dags/{today}/splits/train.csv', index=False)\n",
    "    test.to_csv(f'dags/{today}/splits/test.csv', index=False)\n",
    "\n",
    "# 4. Entrenar modelo\n",
    "def train_model(model_class):\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    train_data = pd.read_csv(f'dags/{today}/splits/train.csv')\n",
    "    X = train_data.drop('HiringDecision', axis=1)\n",
    "    y = train_data['HiringDecision']\n",
    "\n",
    "    numeric_features = ['Age', 'ExperienceYears', 'DistanceFromCompany', \n",
    "                        'SkillScore', 'PersonalityScore']\n",
    "    categorical_features = ['Gender', 'EducationLevel', \n",
    "                            'PreviousCompanies', 'InterviewScore', \n",
    "                            'RecruitmentStrategy']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model_class)\n",
    "    ])\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    model_name = model_class.__class__.__name__\n",
    "    model_path = f'dags/{today}/models/{model_name}.joblib'\n",
    "    dump(clf, model_path)\n",
    "    return model_path\n",
    "\n",
    "# 5. Evaluar modelos\n",
    "def evaluate_models():\n",
    "    today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "    test_data = pd.read_csv(f'dags/{today}/splits/test.csv')\n",
    "    X_test = test_data.drop('HiringDecision', axis=1)\n",
    "    y_test = test_data['HiringDecision']\n",
    "    \n",
    "    model_dir = f'dags/{today}/models'\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_model_name = ''\n",
    "    \n",
    "    for model_file in os.listdir(model_dir):\n",
    "        model_path = os.path.join(model_dir, model_file)\n",
    "        model = load(model_path)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_model_name = model_file\n",
    "    \n",
    "    print(f\"Mejor modelo: {best_model_name} con accuracy: {best_accuracy}\")\n",
    "    dump(best_model, f'dags/{today}/models/best_model.joblib')\n",
    "\n",
    "# ejecutamos secuencialmente\n",
    "if __name__ == \"__main__\":\n",
    "    create_folders()\n",
    "    load_and_merge()\n",
    "    split_data()\n",
    "    train_model(RandomForestClassifier(random_state=42))\n",
    "    train_model(LogisticRegression(random_state=42))\n",
    "    evaluate_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrmM65RIRrgm"
   },
   "source": [
    "# Conclusi√≥n\n",
    "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por el foro de U-cursos o por correo.\n",
    "\n",
    "<center>\n",
    "<img src =\"https://media0.giphy.com/media/W12WAzuqod9VS/200w.gif?cid=6c09b952gekq3fm1no1ttwcvgm9oj3khbm6yxbe6qwnx3nad&ep=v1_gifs_search&rid=200w.gif&ct=g\" width = 400 />\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
